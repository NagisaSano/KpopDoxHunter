import requests, re, pandas as pd, numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime

API_KEY = "AIzaSyBWRz48ILesDgGdUZTPsUbdRFkeXz35gvU."  # Ta clÃ©
QUERIES = ["Hamedaxmj Felix", "Felix SÃ©oul", "Stray Kids maison", "Felix CorÃ©e address"]

# ML Dataset rÃ©el (dox patterns)
DOX_CORPUS = [
    "Felix maison SÃ©oul transports 25 minutes", "Hamedaxmj devant chez Felix Stray Kids",
    "adresse Felix quartier CorÃ©e du Sud", "Felix lives here Seoul house passants"
]

def ml_dox_hunter():
    vectorizer = TfidfVectorizer(stop_words='english')
    X_train = vectorizer.fit_transform(DOX_CORPUS)
    
    results = []
    for query in QUERIES:
        url = f"https://www.googleapis.com/youtube/v3/search"
        params = {'part': 'snippet', 'q': query, 'type': 'video', 'key': API_KEY, 'maxResults': 15}
        
        resp = requests.get(url, params=params).json()
        for video in resp['items']:
            text = (video['snippet']['title'] + ' ' + video['snippet']['description']).lower()
            vec = vectorizer.transform([text])
            dox_score = cosine_similarity(X_train, vec).max()
            
            results.append({
                'query': query, 'title': video['snippet']['title'][:100],
                'video_id': video['id']['videoId'], 'dox_score': dox_score,
                'timestamp': datetime.now()
            })
    
    df = pd.DataFrame(results)
    df = df[df['dox_score'] > 0.1].sort_values('dox_score', ascending=False)
    filename = f"dox_report_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
    df.to_csv(filename, index=False)
    print(f"ðŸŒ‹ V8 ML SCAN: {len(df)} hits â†’ {filename}")
    print(df[['title', 'dox_score']].head())
    return df

if __name__ == "__main__":
    hits = ml_dox_hunter()
